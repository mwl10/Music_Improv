{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43502353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088c9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:  list of file names\n",
    "\n",
    "# output: a list of strings where chords are a grouping of numerical notes, \n",
    "#         separated by '.'; notes are the note names themselves\n",
    "def file_to_note_str(files):\n",
    "    notes = []\n",
    "    for file in files:\n",
    "        # load file into Music21 stream to get list of all the notes and chords\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = None\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "        # melodies must be in the same position! (Here the bass lines are in index 2 of the midi)\n",
    "        notes_to_parse = parts.parts[2].recurse()\n",
    "        for elem in notes_to_parse:\n",
    "            if isinstance(elem, note.Note):\n",
    "                notes.append(str(elem.pitch))\n",
    "            elif isinstance(elem, chord.Chord):\n",
    "                # append chords by encoding the id of every note in the chord together in a string separated by a dot\n",
    "                notes.append('.'.join(str(n) for n in elem.normalOrder))\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7258c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input sequences and corresponding output for the rnn\n",
    "\n",
    "def prep_sequences(notes, sequence_length = 20):\n",
    "    # pitchnames holds all the different notes/chords in a set\n",
    "    pitchnames = sorted(set(notes))\n",
    "    n_vocab = len(pitchnames)\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    input= []\n",
    "    output = []\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "    num_patterns = len(input)\n",
    "    input = np.reshape(input, (num_patterns, sequence_length, 1))\n",
    "    # normalize\n",
    "    input = input / float(n_vocab)\n",
    "    \n",
    "    output = to_categorical(output)\n",
    "    print('pitchnames:\\n', pitchnames)\n",
    "    print('number of notes/chords\\n', n_vocab)\n",
    "    return input, output, n_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2417ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model \n",
    "def create_model(input, n_vocab):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(256, input_shape=(input.shape[1], input.shape[2]), return_sequences=True))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(LSTM(512, return_sequences=True))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(LSTM(256))\n",
    "#     model.add(Dense(256))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(n_vocab))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(input.shape[1], input.shape[2]), recurrent_dropout=0.3, return_sequences=True))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea449cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(512, input_shape=(input.shape[1], input.shape[2]), recurrent_dropout=0.3, return_sequences=True))\n",
    "# model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "# model.add(LSTM(512))\n",
    "# model.add(BatchNorm())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNorm())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(n_vocab))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e73152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model based on the data, and output checkpoints that you can monitor and input to make predictions \n",
    "def train(model, input, output, epochs=100, batch_size=64):\n",
    "    #print(input)\n",
    "    fp = \"weight_checkpoints/music/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(fp, monitor='loss', verbose=0,save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(input, output, epochs=100, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31714daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"bass_mids/*.MID\")\n",
    "def train_rnn(files):\n",
    "    notes = file_to_note_str(files)\n",
    "    input, output, n_vocab = prep_sequences(notes)\n",
    "    model = create_model(input, n_vocab)\n",
    "    print(input.shape)\n",
    "    train(model, input, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cb967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitchnames:\n",
      " ['0.4', '0.5', '1.6', '10.11', '2.6', '3.6', '4.10', '4.6', '6.11', '7.0', 'A3', 'A4', 'B-2', 'B-3', 'B-4', 'B-5', 'B2', 'B3', 'C#4', 'C4', 'C5', 'C6', 'D3', 'D4', 'D5', 'D6', 'E-4', 'E-5', 'E5', 'F#2', 'F#4', 'F4', 'F5', 'G#4', 'G3', 'G4', 'G5']\n",
      "number of notes/chords\n",
      " 37\n",
      "(4855, 20, 1)\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x17d8c5e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x17d8c5e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 22:12:38.826538: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-14 22:12:38.826677: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 47s 605ms/step - loss: 3.5447\n",
      "Epoch 2/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 3.1735\n",
      "Epoch 3/100\n",
      "76/76 [==============================] - 46s 604ms/step - loss: 2.6267\n",
      "Epoch 4/100\n",
      "76/76 [==============================] - 47s 612ms/step - loss: 2.3143\n",
      "Epoch 5/100\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 2.0762\n",
      "Epoch 6/100\n",
      "76/76 [==============================] - 47s 612ms/step - loss: 1.8145\n",
      "Epoch 7/100\n",
      "76/76 [==============================] - 47s 621ms/step - loss: 1.6012\n",
      "Epoch 8/100\n",
      "76/76 [==============================] - 48s 635ms/step - loss: 1.5101\n",
      "Epoch 9/100\n",
      "76/76 [==============================] - 47s 616ms/step - loss: 1.3506\n",
      "Epoch 10/100\n",
      "76/76 [==============================] - 47s 612ms/step - loss: 1.1992\n",
      "Epoch 11/100\n",
      "76/76 [==============================] - 48s 633ms/step - loss: 1.0727\n",
      "Epoch 12/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 1.0382\n",
      "Epoch 13/100\n",
      "76/76 [==============================] - 48s 634ms/step - loss: 0.8868\n",
      "Epoch 14/100\n",
      "76/76 [==============================] - 47s 624ms/step - loss: 0.8602\n",
      "Epoch 15/100\n",
      "76/76 [==============================] - 47s 623ms/step - loss: 0.9169\n",
      "Epoch 16/100\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 0.7306\n",
      "Epoch 17/100\n",
      "76/76 [==============================] - 47s 617ms/step - loss: 0.6218\n",
      "Epoch 18/100\n",
      "76/76 [==============================] - 46s 610ms/step - loss: 0.6064\n",
      "Epoch 19/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 0.5249\n",
      "Epoch 20/100\n",
      "76/76 [==============================] - 47s 618ms/step - loss: 0.6847\n",
      "Epoch 21/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 0.5907\n",
      "Epoch 22/100\n",
      "76/76 [==============================] - 47s 617ms/step - loss: 0.5167\n",
      "Epoch 23/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 0.4764\n",
      "Epoch 24/100\n",
      "76/76 [==============================] - 47s 624ms/step - loss: 0.3808\n",
      "Epoch 25/100\n",
      "76/76 [==============================] - 48s 627ms/step - loss: 0.4153\n",
      "Epoch 26/100\n",
      "76/76 [==============================] - 48s 638ms/step - loss: 0.4071\n",
      "Epoch 27/100\n",
      "76/76 [==============================] - 48s 627ms/step - loss: 0.3907\n",
      "Epoch 28/100\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 0.4510\n",
      "Epoch 29/100\n",
      "76/76 [==============================] - 47s 618ms/step - loss: 0.3264\n",
      "Epoch 30/100\n",
      "76/76 [==============================] - 47s 614ms/step - loss: 0.3019\n",
      "Epoch 31/100\n",
      "76/76 [==============================] - 47s 623ms/step - loss: 0.3600\n",
      "Epoch 32/100\n",
      "76/76 [==============================] - 47s 623ms/step - loss: 0.2779\n",
      "Epoch 33/100\n",
      "76/76 [==============================] - 47s 615ms/step - loss: 0.2792\n",
      "Epoch 34/100\n",
      "76/76 [==============================] - 46s 602ms/step - loss: 0.2680\n",
      "Epoch 35/100\n",
      "76/76 [==============================] - 46s 606ms/step - loss: 0.3682\n",
      "Epoch 36/100\n",
      "76/76 [==============================] - 46s 607ms/step - loss: 0.3113\n",
      "Epoch 37/100\n",
      "76/76 [==============================] - 50s 660ms/step - loss: 0.2375\n",
      "Epoch 38/100\n",
      "76/76 [==============================] - 49s 651ms/step - loss: 0.3366\n",
      "Epoch 39/100\n",
      "76/76 [==============================] - 48s 626ms/step - loss: 0.2325\n",
      "Epoch 40/100\n",
      "76/76 [==============================] - 47s 621ms/step - loss: 0.2371\n",
      "Epoch 41/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.2386\n",
      "Epoch 42/100\n",
      "76/76 [==============================] - 48s 638ms/step - loss: 0.2233\n",
      "Epoch 43/100\n",
      "76/76 [==============================] - 49s 643ms/step - loss: 0.2284\n",
      "Epoch 44/100\n",
      "76/76 [==============================] - 49s 643ms/step - loss: 0.2610\n",
      "Epoch 45/100\n",
      "76/76 [==============================] - 47s 616ms/step - loss: 0.1975\n",
      "Epoch 46/100\n",
      "76/76 [==============================] - 46s 607ms/step - loss: 0.2665\n",
      "Epoch 47/100\n",
      "76/76 [==============================] - 46s 603ms/step - loss: 0.1764\n",
      "Epoch 48/100\n",
      "76/76 [==============================] - 47s 612ms/step - loss: 0.1749\n",
      "Epoch 49/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.2181\n",
      "Epoch 50/100\n",
      "76/76 [==============================] - 47s 617ms/step - loss: 0.1558\n",
      "Epoch 51/100\n",
      "76/76 [==============================] - 47s 616ms/step - loss: 0.1778\n",
      "Epoch 52/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.2238\n",
      "Epoch 53/100\n",
      "76/76 [==============================] - 47s 616ms/step - loss: 0.1980\n",
      "Epoch 54/100\n",
      "76/76 [==============================] - 46s 610ms/step - loss: 0.1697\n",
      "Epoch 55/100\n",
      "76/76 [==============================] - 47s 618ms/step - loss: 0.1566\n",
      "Epoch 56/100\n",
      "76/76 [==============================] - 48s 635ms/step - loss: 0.2026\n",
      "Epoch 57/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.1553\n",
      "Epoch 58/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 0.1428\n",
      "Epoch 59/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.1501\n",
      "Epoch 60/100\n",
      "76/76 [==============================] - 46s 609ms/step - loss: 0.1356\n",
      "Epoch 61/100\n",
      "76/76 [==============================] - 45s 599ms/step - loss: 0.1459\n",
      "Epoch 62/100\n",
      "76/76 [==============================] - 46s 607ms/step - loss: 0.2225\n",
      "Epoch 63/100\n",
      "76/76 [==============================] - 46s 601ms/step - loss: 0.1383\n",
      "Epoch 64/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.1276\n",
      "Epoch 65/100\n",
      "76/76 [==============================] - 48s 629ms/step - loss: 0.1206\n",
      "Epoch 66/100\n",
      "76/76 [==============================] - 47s 615ms/step - loss: 0.1330\n",
      "Epoch 67/100\n",
      "76/76 [==============================] - 46s 607ms/step - loss: 0.1350\n",
      "Epoch 68/100\n",
      "76/76 [==============================] - 46s 604ms/step - loss: 0.1109\n",
      "Epoch 69/100\n",
      "76/76 [==============================] - 46s 606ms/step - loss: 0.1755\n",
      "Epoch 70/100\n",
      "76/76 [==============================] - 46s 601ms/step - loss: 0.1251\n",
      "Epoch 71/100\n",
      "76/76 [==============================] - 46s 600ms/step - loss: 0.1238\n",
      "Epoch 72/100\n",
      "76/76 [==============================] - 46s 601ms/step - loss: 0.1069\n",
      "Epoch 73/100\n",
      "76/76 [==============================] - 46s 600ms/step - loss: 0.1139\n",
      "Epoch 74/100\n",
      "76/76 [==============================] - 46s 608ms/step - loss: 0.0981\n",
      "Epoch 75/100\n",
      "76/76 [==============================] - 46s 600ms/step - loss: 0.1145\n",
      "Epoch 76/100\n",
      "76/76 [==============================] - 46s 606ms/step - loss: 0.1228\n",
      "Epoch 77/100\n",
      "76/76 [==============================] - 46s 608ms/step - loss: 0.0984\n",
      "Epoch 78/100\n",
      "76/76 [==============================] - 46s 600ms/step - loss: 0.1023\n",
      "Epoch 79/100\n",
      "76/76 [==============================] - 47s 623ms/step - loss: 0.1245\n",
      "Epoch 80/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.1177\n",
      "Epoch 81/100\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 0.1110\n",
      "Epoch 82/100\n",
      "76/76 [==============================] - 46s 604ms/step - loss: 0.1134\n",
      "Epoch 83/100\n",
      "76/76 [==============================] - 47s 625ms/step - loss: 0.1271\n",
      "Epoch 84/100\n",
      "76/76 [==============================] - 47s 619ms/step - loss: 0.1338\n",
      "Epoch 85/100\n",
      "76/76 [==============================] - 46s 602ms/step - loss: 0.0988\n",
      "Epoch 86/100\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 0.0949\n",
      "Epoch 87/100\n",
      "76/76 [==============================] - 47s 613ms/step - loss: 0.1136\n",
      "Epoch 88/100\n",
      "76/76 [==============================] - 47s 623ms/step - loss: 0.0837\n",
      "Epoch 89/100\n",
      "76/76 [==============================] - 48s 636ms/step - loss: 0.1072\n",
      "Epoch 90/100\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 0.1108\n",
      "Epoch 91/100\n",
      "76/76 [==============================] - 48s 631ms/step - loss: 0.0986\n",
      "Epoch 92/100\n",
      "76/76 [==============================] - 46s 609ms/step - loss: 0.1012\n",
      "Epoch 93/100\n",
      "76/76 [==============================] - 48s 631ms/step - loss: 0.1201\n",
      "Epoch 94/100\n",
      "76/76 [==============================] - 47s 622ms/step - loss: 0.1335\n",
      "Epoch 95/100\n",
      "76/76 [==============================] - 46s 606ms/step - loss: 0.1022\n",
      "Epoch 96/100\n",
      "76/76 [==============================] - 47s 616ms/step - loss: 0.0868\n",
      "Epoch 97/100\n",
      "76/76 [==============================] - 47s 615ms/step - loss: 0.1166\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 49s 646ms/step - loss: 0.1009\n",
      "Epoch 99/100\n",
      "76/76 [==============================] - 48s 636ms/step - loss: 0.1063\n",
      "Epoch 100/100\n",
      "76/76 [==============================] - 47s 619ms/step - loss: 0.0921\n"
     ]
    }
   ],
   "source": [
    "train_rnn(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb29b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitchnames:\n",
      " ['0.4', '0.5', '1.6', '10.11', '2.6', '3.6', '4.10', '4.6', '6.11', '7.0', 'A3', 'A4', 'B-2', 'B-3', 'B-4', 'B-5', 'B2', 'B3', 'C#4', 'C4', 'C5', 'C6', 'D3', 'D4', 'D5', 'D6', 'E-4', 'E-5', 'E5', 'F#2', 'F#4', 'F4', 'F5', 'G#4', 'G3', 'G4', 'G5']\n",
      "number of notes/chords\n",
      " 37\n"
     ]
    }
   ],
   "source": [
    "notes = file_to_note_str(files)\n",
    "input, output, n_vocab = prep_sequences(notes)\n",
    "pitchnames = sorted(set(notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f82b1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(input.shape[1], input.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(512, return_sequences=True))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(n_vocab))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.load_weights(\"weight_checkpoints/music/weights-improvement-109-0.0711.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a840a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated model \n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(input.shape[1], input.shape[2]), recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "model.add(LSTM(512))\n",
    "model.add(BatchNorm())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNorm())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.load_weights(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af46f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x17a6c19d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x17a6c19d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(input) -1)\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "pattern = input[start]\n",
    "pred_output = []\n",
    "for note_index in range(500):\n",
    "    # make it 1,50,1 instead of 50,1\n",
    "    pred_input = np.reshape(pattern, (1,len(pattern), 1))\n",
    "    # normalize input\n",
    "    pred_input = pred_input / float(n_vocab)\n",
    "    # predict w/ model\n",
    "    pred = model.predict(pred_input, verbose=0)\n",
    "    # get the most likely index of the next note => can change this to any somewhat likely index for more spontaneity\n",
    "    index = np.argmax(pred)\n",
    "    # convert to note form \n",
    "    result = int_to_note[index]\n",
    "    # append the result to the output\n",
    "    pred_output.append(result)\n",
    "    pattern = np.append(pattern,index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8dcd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A4', 'A4', 'A4', 'A4', 'A4', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'B-2', '4.10', 'B-2', '4.10', 'B-2', '2.6', 'D3', '2.6', 'D3', '6.11', 'B2', '6.11', 'B2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2', '4.6', 'F#2']\n"
     ]
    }
   ],
   "source": [
    "print(pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e26ce5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate chords/notes\n",
    "offset = 0\n",
    "output_notes = []\n",
    "for pattern in pred_output:\n",
    "    # for chords\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        chord_notes = pattern.split('.')\n",
    "        notes = []\n",
    "        for cur_note in chord_notes:\n",
    "            n_note = note.Note(int(cur_note))\n",
    "            n_note.storeInstrument = instrument.Bass()\n",
    "            notes.append(n_note)\n",
    "        n_chord = chord.Chord(notes)\n",
    "        n_chord.offset = offset\n",
    "        output_notes.append(n_chord)\n",
    "    # for notes (mostly all w/ the bass)\n",
    "    else:\n",
    "        n_note = note.Note(pattern)\n",
    "        n_note.offset = offset\n",
    "        n_note.storedInstrument = instrument.Bass()\n",
    "        output_notes.append(n_note)\n",
    "    # increase offset \n",
    "    offset+=0.5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e69cf9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/test_out4.mid'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='output/test_out4.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b808b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid2wav('output/my_music.midi')\n",
    "# IPython.display.Audio('./output/rendered.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ac883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3bbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86a28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5aafca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae9258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec590e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab8acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
